{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Ea7BFPBy3RtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv6MkjZFuP7b",
        "outputId": "6ac744ff-842d-4ae8-d74a-65cc02aefcc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfG3bM75nM8U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/데이터_전처리_파일_선박명.csv', encoding='cp949')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Upr1OQEpI_l",
        "outputId": "7357043d-e0b7-4366-9653-caf62a1fca19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G67pkbOkQ4Q"
      },
      "source": [
        "대분류, 중분류, 소분류를 feature 값으로 사용하고, target 값으로는 COOLER, GASKET, GEAR, O-RING 등 61개 분류명을 사용하여 다중 분류 모델을 만들 수 있습니다.\n",
        "\n",
        "전처리 과정에서는 대분류, 중분류, 소분류를 텍스트로 받아와서 토큰화하여 리스트 형태로 만들고, target 값을 숫자로 인코딩하여 넘파이 배열 형태로 만들어줍니다. 그 후, 해당 데이터를 분류 모델에 학습시켜 예측 모델을 만들어서 새로운 입력 데이터에 대해 분류할 수 있습니다.\n",
        "\n",
        "예를 들어, 다음과 같은 코드로 분류 모델을 만들 수 있습니다. (사용하는 머신러닝 라이브러리와 분류 알고리즘에 따라 코드는 달라질 수 있습니다.)\n",
        "\n",
        " CountVectorizer를 사용하여 텍스트 데이터를 feature 벡터로 변환하고, LabelEncoder를 이용하여 target 값을 숫자로 인코딩하였습니다. 이후 train_test_split 함수를 사용하여 데이터를 train, test 데이터로 분리하고, OneVsRestClassifier를 사용하여 SVM 분류기를 만들어 분류 모델을 학습시켰습니다. 마지막으로, 새로운 입력 데이터를 예측하여 target 값을 출력하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf58PNEfjccV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEzoKGfgje_9"
      },
      "outputs": [],
      "source": [
        "# 텍스트 데이터\n",
        "data = list(df['Subject'] + ' '+ df['Machinery']  + ' ' + df['Assembly'] + ' ' + df['청구품목'] + ' ' + df['Part No.1'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFnUJRwvjm3z"
      },
      "outputs": [],
      "source": [
        "# target 값\n",
        "target =list(df['key2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlq7mjLij186"
      },
      "outputs": [],
      "source": [
        "# CountVectorizer를 이용하여 feature 벡터 생성\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjuU-Utvj6jL"
      },
      "outputs": [],
      "source": [
        "# LabelEncoder를 이용하여 target 값을 숫자로 인코딩\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf-Vw5Aoj_Dk"
      },
      "outputs": [],
      "source": [
        "# train, test 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCY22Bkd3PBN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score # 정확도 함수"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score\n",
        "def print_metrics(y_test, X_test, model) :\n",
        "  # 테스트 데이터셋으로 예측\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # 각종 평가지표 계산\n",
        "  recall = recall_score(y_test, y_pred, average='macro')\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='macro')\n",
        "  f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "  print(\"Recall: {:.4f}\".format(recall))\n",
        "  print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"Precision: {:.4f}\".format(precision))\n",
        "  print(\"F1-score: {:.4f}\".format(f1))\n"
      ],
      "metadata": {
        "id": "vmCA-Lgmx9dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression 모델 생성 및 학습\n",
        "lr = LogisticRegression(random_state=42, C=8, solver=\"liblinear\", multi_class=\"ovr\", max_iter=150)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "print_metrics(y_test, X_test, lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pEvwqKrX2Zz",
        "outputId": "b8defc28-da69-4108-89f3-b3bb2eeba281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.8341\n",
            "Accuracy: 0.8908\n",
            "Precision: 0.9083\n",
            "F1-score: 0.8594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피클로 모델 저장\n",
        "\n",
        "import pickle \n",
        "with open('model.pickle','wb') as fw:\n",
        "    pickle.dump(lr, fw)"
      ],
      "metadata": {
        "id": "RW_P3U1ARNhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer 생성\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# feature 벡터 생성\n",
        "X = vectorizer.fit_transform(data)\n",
        "\n",
        "# vectorizer 객체 저장\n",
        "with open('vectorizer.pickle', 'wb') as fw:\n",
        "    pickle.dump(vectorizer, fw)"
      ],
      "metadata": {
        "id": "Z-wUr4iNlDNM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LabelEncoder 생성 및 target 인코딩\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(target)\n",
        "\n",
        "# LabelEncoder 객체 저장\n",
        "with open('label_encoder.pickle', 'wb') as fw:\n",
        "    pickle.dump(le, fw)"
      ],
      "metadata": {
        "id": "K1BcFuT3lSu5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오기\n",
        "\n",
        "import pickle \n",
        "with open('model.pickle', 'rb') as f: \n",
        "    model = pickle.load(f)"
      ],
      "metadata": {
        "id": "Ta53THUXRdwr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}